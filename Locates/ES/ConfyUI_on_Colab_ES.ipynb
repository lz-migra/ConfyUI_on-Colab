{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "erGgBwjKS9Rd"
      },
      "outputs": [],
      "source": [
        "#@title # üõ†Ô∏è Panel de Configuraci√≥n de ComfyUI\n",
        "#@markdown ---\n",
        "#@markdown ### üìÅ Gesti√≥n de Almacenamiento (Google Drive)\n",
        "#@markdown Elige c√≥mo quieres manejar tus archivos. Montar Drive permite que tus modelos y resultados no se borren al cerrar Colab.\n",
        "MODO_DRIVE = \"SOLO_LOCAL\" #@param [\"MONTAR\", \"DESMONTAR\", \"SOLO_LOCAL\"]\n",
        "USAR_DRIVE_PARA_COMFY = True #@param {type:\"boolean\"}\n",
        "#@markdown > **Nota:** Si activas la casilla de arriba, ComfyUI se instalar√° en `/content/drive/MyDrive/AI/ComfyUI`. Esto ahorra tiempo en futuras sesiones ya que no tendr√°s que re-descargar todo.\n",
        "RUTA_PERSONALIZADA_DRIVE = \"AI/ComfyUI\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üîÑ Actualizaciones y Mantenimiento\n",
        "ACTUALIZAR_COMFY = True #@param {type:\"boolean\"}\n",
        "#@markdown > **Actualizar:** Descarga la versi√≥n m√°s reciente de ComfyUI desde GitHub autom√°ticamente.\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üß† Instalaci√≥n de Modelos e Inteligencia Artificial\n",
        "#@markdown Selecciona qu√© modelos quieres pre-instalar para empezar a trabajar de inmediato.\n",
        "INSTALAR_QWEN_VL = False #@param {type:\"boolean\"}\n",
        "#@markdown > **Edici√≥n de imagenes Qwen VL/2509:** Instala los modelos necesarios para usar estos Workflows.\n",
        "#@markdown >\n",
        "#@markdown > Un modelo multimodal avanzado capaz de \"entender\" im√°genes y editarlas siguiendo instrucciones complejas. Requiere nodos espec√≠ficos que se instalar√°n autom√°ticamente.\n",
        "#@markdown >\n",
        "#@markdown > <font color='red'>Nota: Este modelo es muy pesado por lo que las maquinas free de Google Colab no suelen aguantarlo. (Se detiene por falta de RAM).</font>\n",
        "\n",
        "INSTALAR_SD15_BASE = False #@param {type:\"boolean\"}\n",
        "#@markdown > **Stable Diffusion 1.5:** El est√°ndar de oro para generaci√≥n de im√°genes. Incluye el modelo base y un VAE para mejorar colores y rostros.\n",
        "\n",
        "INSTALAR_PROTOVISION_XL = False #@param {type:\"boolean\"}\n",
        "INSTALAR_JUGGERNAUT_XL_HYPER = False #@param {type:\"boolean\"}\n",
        "INSTALAR_JUGGERNAUT_X = False #@param {type:\"boolean\"}\n",
        "INSTALAR_JUGGERNAUT_V8 = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üì¶ Instalar Modelos Personalizados\n",
        "#@markdown Ingresa la lista de modelos en formato JSON.\n",
        "#@markdown > Puedes generar la cadena JSON con el formato correcto a trav√©s del siguiente proyecto de Google Apps Script:\n",
        "#@markdown > ### [üëâ Haz clic AQU√ç para abrir la herramienta](https://script.google.com/macros/s/AKfycbzmfmxfFRKvlnwlDuHNwOhWqEzYd90f1YKaQxSuFQ3o8HyQW_oHZZgPo9j4vUQlJ7ZI/exec)\n",
        "\n",
        "MODELOS_PERSONALIZADOS = \"[]\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üöÄ Configuraci√≥n de Ejecuci√≥n\n",
        "\n",
        "USAR_LOWVRAM = False #@param {type:\"boolean\"}\n",
        "#@markdown > **üìâ Low VRAM:** Act√≠valo si Colab se cierra inesperadamente o te quedas sin memoria. Ayuda a gestionar mejor los recursos.\n",
        "\n",
        "USAR_CPU_ONLY = False #@param {type:\"boolean\"}\n",
        "#@markdown > **üêå Solo CPU:** Act√≠valo si no tienes acceso a una GPU. La generaci√≥n ser√° extremadamente lenta.\n",
        "\n",
        "TIPO_TUNEL = \"LOCALTUNNEL\" #@param [\"CLOUDFLARE\", \"LOCALTUNNEL\"]\n",
        "#@markdown > **üåê Tipo de T√∫nel:** Selecciona el m√©todo de conexi√≥n para acceder a la interfaz web.\n",
        "\n",
        "ARGUMENTOS_ADICIONALES = \"\" #@param {type:\"string\"}\n",
        "#@markdown > **‚å®Ô∏è Argumentos Adicionales:** Inserta comandos extra para modificar el comportamiento del programa.\n",
        "\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import os\n",
        "import torch\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    if not USAR_CPU_ONLY:\n",
        "        print(\"\\n\" + \"!\"*60)\n",
        "        print(\"‚ùå ERROR CR√çTICO: NO SE DETECT√ì GPU\")\n",
        "        print(\"------------------------------------------------------------\")\n",
        "        print(\"ComfyUI requiere una GPU para funcionar. Actualmente est√°s usando CPU.\")\n",
        "        print(\"üëâ Ve a: Entorno de ejecuci√≥n > Cambiar tipo de entorno de ejecuci√≥n\")\n",
        "        print(\"üëâ Selecciona 'T4 GPU' en Acelerador de hardware.\")\n",
        "        print(\"!\"*60 + \"\\n\")\n",
        "        raise RuntimeError(\"Debes seleccionar un entorno con GPU (T4) para continuar.\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è EJECUTANDO EN MODO CPU: La generaci√≥n de im√°genes ser√° muy lenta.\")\n",
        "\n",
        "if MODO_DRIVE == \"MONTAR\":\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "elif MODO_DRIVE == \"DESMONTAR\":\n",
        "  try:\n",
        "    drive.flush_and_unmount()\n",
        "  except ValueError:\n",
        "    pass\n",
        "  get_ipython().system_raw(\"rm -rf /root/.config/Google/DriveFS\")\n",
        "\n",
        "WORKSPACE = '/content/ComfyUI'\n",
        "if MODO_DRIVE == \"MONTAR\" and USAR_DRIVE_PARA_COMFY:\n",
        "    WORKSPACE = \"/content/drive/MyDrive/\" + RUTA_PERSONALIZADA_DRIVE\n",
        "\n",
        "if not os.path.exists(WORKSPACE):\n",
        "    print(\"-= Initial setup ComfyUI =-\")\n",
        "    !git clone https://github.com/comfyanonymous/ComfyUI {WORKSPACE}\n",
        "else:\n",
        "    if ACTUALIZAR_COMFY:\n",
        "        print(\"-= Updating ComfyUI =-\")\n",
        "        %cd {WORKSPACE}\n",
        "        !git pull\n",
        "\n",
        "%cd {WORKSPACE}\n",
        "\n",
        "!echo -= Install dependencies =-\n",
        "!pip install xformers!=0.0.18 -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://download.pytorch.org/whl/cu118 --extra-index-url https://download.pytorch.org/whl/cu117\n",
        "\n",
        "%cd {WORKSPACE}\n",
        "\n",
        "\n",
        "\n",
        "%cd {WORKSPACE}\n",
        "# Install Aria2\n",
        "!apt-get -y install -qq aria2\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "CIVITAI_API_TOKEN = userdata.get('CIVITAI_API_TOKEN')\n",
        "print(\"Loaded API key:\", \"‚úÖ\" if CIVITAI_API_TOKEN else \"‚ùå Not found\")\n",
        "\n",
        "# Install custom nodes\n",
        "\n",
        "def install_custom_node(url):\n",
        "  %cd {WORKSPACE}/custom_nodes\n",
        "  !git clone {url}\n",
        "\n",
        "def downloadModel(url, filename = None):\n",
        "  if 'huggingface.co' in url:\n",
        "    if filename is None:\n",
        "      filename = url.split('/')[-1]\n",
        "      filename = filename.removesuffix('?download=true')\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {url}  -o {filename}\n",
        "  else:\n",
        "    # civitai\n",
        "    if filename:\n",
        "      !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {url}?token={CIVITAI_API_TOKEN} -o {filename}\n",
        "    else:\n",
        "      !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {url}?token={CIVITAI_API_TOKEN}\n",
        "\n",
        "\n",
        "# install custom nodes\n",
        "#install_custom_node('https://github.com/ltdrdata/ComfyUI-Manager.git')\n",
        "#install_custom_node('https://github.com/ltdrdata/ComfyUI-Impact-Pack')\n",
        "\n",
        "\n",
        "# download models\n",
        "%cd ./models/checkpoints\n",
        "\n",
        "# CyberRealistic Pony\n",
        "downloadModel('https://civitai.com/api/download/models/2071650')\n",
        "\n",
        "# Protovision XL\n",
        "if INSTALAR_PROTOVISION_XL:\n",
        "    downloadModel('https://civitai.com/api/download/models/201514')\n",
        "# Juggernaut XL Hyper\n",
        "if INSTALAR_JUGGERNAUT_XL_HYPER:\n",
        "    downloadModel('https://civitai.com/api/download/models/471120')\n",
        "# Juppernaut X\n",
        "if INSTALAR_JUGGERNAUT_X:\n",
        "    downloadModel('https://civitai.com/api/download/models/456194')\n",
        "# Juggernaut v8\n",
        "if INSTALAR_JUGGERNAUT_V8:\n",
        "    downloadModel('https://civitai.com/api/download/models/288982')\n",
        "\n",
        "%cd {WORKSPACE}\n",
        "\n",
        "\n",
        "\n",
        "if INSTALAR_QWEN_VL:\n",
        "    # 1. Configuraci√≥n de carpetas y descargas de Qwen\n",
        "    # ----------------------------------------------------------------\n",
        "\n",
        "    # VAE\n",
        "    %cd {WORKSPACE}/models/vae\n",
        "    downloadModel('https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/vae/qwen_image_vae.safetensors')\n",
        "\n",
        "    # DIFFUSION MODELS (Unet)\n",
        "    # Nota: Si la carpeta no existe, la creamos\n",
        "    !mkdir -p {WORKSPACE}/models/diffusion_models\n",
        "    %cd {WORKSPACE}/models/diffusion_models\n",
        "    downloadModel('https://huggingface.co/Comfy-Org/Qwen-Image-Edit_ComfyUI/resolve/main/split_files/diffusion_models/qwen_image_edit_fp8_e4m3fn.safetensors')\n",
        "\n",
        "    # TEXT ENCODERS\n",
        "    !mkdir -p {WORKSPACE}/models/text_encoders\n",
        "    %cd {WORKSPACE}/models/text_encoders\n",
        "    downloadModel('https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/text_encoders/qwen_2.5_vl_7b_fp8_scaled.safetensors')\n",
        "\n",
        "    # LORAS\n",
        "    %cd {WORKSPACE}/models/loras\n",
        "    downloadModel('https://huggingface.co/lightx2v/Qwen-Image-Lightning/resolve/main/Qwen-Image-Edit-Lightning-4steps-V1.0-bf16.safetensors')\n",
        "\n",
        "    # 2. Instalaci√≥n del Nodo Personalizado (Obligatorio para que funcione el workflow)\n",
        "    # ----------------------------------------------------------------\n",
        "    %cd {WORKSPACE}/custom_nodes\n",
        "    # Clonamos el repositorio oficial que maneja estos modelos de Qwen\n",
        "    !git clone https://github.com/Comfy-Org/ComfyUI-Qwen-VL-API.git 2>/dev/null || echo \"üü¢ Edici√≥n de imagen de Qwen/2095 - Instalacion Finalizada\"\n",
        "\n",
        "    # Volver al inicio\n",
        "    %cd {WORKSPACE}\n",
        "\n",
        "if INSTALAR_SD15_BASE:\n",
        "    # --- Descarga de Modelos SD 1.5 (Optimizados) ---\n",
        "\n",
        "    # SD1.5 Pruned (Versi√≥n Safetensors - M√°s r√°pida y segura)\n",
        "    %cd {WORKSPACE}/models/checkpoints\n",
        "    downloadModel('https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors')\n",
        "\n",
        "    # AbyssOrangeMix2 (Anime Style)\n",
        "    downloadModel('https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_hard.safetensors')\n",
        "\n",
        "    # VAE Standard (Mejora el color y detalles en SD 1.5)\n",
        "    %cd {WORKSPACE}/models/vae\n",
        "    downloadModel('https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors')\n",
        "\n",
        "    %cd {WORKSPACE}\n",
        "\n",
        "if MODELOS_PERSONALIZADOS:\n",
        "    import json\n",
        "    print(\"‚¨áÔ∏è Descargando modelos personalizados...\")\n",
        "    try:\n",
        "        custom_models = json.loads(MODELOS_PERSONALIZADOS)\n",
        "        if isinstance(custom_models, list):\n",
        "            for model in custom_models:\n",
        "                path_str = model.get(\"ruta\", \"\").strip()\n",
        "                url = model.get(\"link\", \"\").strip()\n",
        "\n",
        "                if path_str and url:\n",
        "                    if \"/\" in path_str:\n",
        "                        folder_part, filename_part = path_str.rsplit(\"/\", 1)\n",
        "                        folder_part = folder_part.strip()\n",
        "                        filename_part = filename_part.strip()\n",
        "\n",
        "                        target_path = f\"{WORKSPACE}/models/{folder_part}\"\n",
        "                        !mkdir -p {target_path}\n",
        "                        %cd {target_path}\n",
        "                        downloadModel(url, filename_part)\n",
        "                    else:\n",
        "                        print(f\"‚ö†Ô∏è Formato incorrecto en ruta: {path_str}\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ö†Ô∏è Error: MODELOS_PERSONALIZADOS no es un JSON v√°lido.\")\n",
        "\n",
        "    %cd {WORKSPACE}\n",
        "\n",
        "\n",
        "!ls -al ./models/checkpoints/\n",
        "\n",
        "\n",
        "if TIPO_TUNEL == \"CLOUDFLARE\":\n",
        "    !wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "    !dpkg -i cloudflared-linux-amd64.deb\n",
        "elif TIPO_TUNEL == \"LOCALTUNNEL\":\n",
        "    !npm install -g localtunnel\n",
        "\n",
        "\n",
        "\n",
        "# --- CREACI√ìN DEL SCRIPT LANZADOR PERSISTENTE ---\n",
        "%cd {WORKSPACE}\n",
        "\n",
        "comfy_args = [\"python\", \"main.py\", \"--dont-print-server\"]\n",
        "if USAR_CPU_ONLY:\n",
        "    comfy_args.append(\"--cpu\")\n",
        "if USAR_LOWVRAM:\n",
        "    comfy_args.append(\"--lowvram\")\n",
        "if ARGUMENTOS_ADICIONALES:\n",
        "    comfy_args.extend(ARGUMENTOS_ADICIONALES.split())\n",
        "\n",
        "launcher_script = f\"\"\"\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import socket\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "def launch_tunnel(port):\n",
        "    print(\"\\\\n[T√∫nel] Esperando a que ComfyUI inicie...\")\n",
        "    while True:\n",
        "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "        if sock.connect_ex(('127.0.0.1', port)) == 0:\n",
        "            sock.close()\n",
        "            break\n",
        "        sock.close()\n",
        "        time.sleep(1)\n",
        "\n",
        "    tunnel_type = \"{TIPO_TUNEL}\"\n",
        "\n",
        "    if tunnel_type == \"CLOUDFLARE\":\n",
        "        proc = subprocess.Popen([\"cloudflared\", \"tunnel\", \"--url\", \"http://127.0.0.1:{{}}\".format(port)],\n",
        "                                stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        for line in proc.stderr:\n",
        "            if \"trycloudflare.com \" in line:\n",
        "                url = line.split(\"http\")[1].strip()\n",
        "                print(\"\\\\n\" + \"=\"*50)\n",
        "                print(f\"üîó URL DE ACCESO: http{{url}}\")\n",
        "                print(\"=\"*50 + \"\\\\n\")\n",
        "\n",
        "    elif tunnel_type == \"LOCALTUNNEL\":\n",
        "        try:\n",
        "            ip = urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\\\n\")\n",
        "            print(\"\\\\n\" + \"=\"*50)\n",
        "            print(f\"üîê PASSWORD/IP LOCALTUNNEL: {{ip}}\")\n",
        "            print(\"=\"*50 + \"\\\\n\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        proc = subprocess.Popen([\"lt\", \"--port\", \"{{}}\".format(port)],\n",
        "                                stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        for line in proc.stdout:\n",
        "            print(line, end='')\n",
        "            if \"your url is\" in line:\n",
        "                url = line.split(\"is:\")[1].strip()\n",
        "                print(\"\\\\n\" + \"=\"*50)\n",
        "                print(f\"üîó URL DE ACCESO: {{url}}\")\n",
        "                print(\"=\"*50 + \"\\\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    threading.Thread(target=launch_tunnel, args=(8188,), daemon=True).start()\n",
        "    subprocess.run({comfy_args})\n",
        "\"\"\"\n",
        "\n",
        "with open(\"colab_launcher.py\", \"w\") as f:\n",
        "    f.write(launcher_script)\n",
        "\n",
        "print(\"\\nüöÄ Iniciando ComfyUI...\")\n",
        "# Ejecutar en segundo plano con nohup para liberar la celda\n",
        "!nohup python -u colab_launcher.py > comfy.log 2>&1 &\n",
        "\n",
        "print(\"‚è≥ Esperando URL de Cloudflare (esto puede tardar unos segundos)...\")\n",
        "import time\n",
        "found = False\n",
        "file_pos = 0\n",
        "while not found:\n",
        "    if os.path.exists(\"comfy.log\"):\n",
        "        with open(\"comfy.log\", \"r\") as f:\n",
        "            f.seek(file_pos)\n",
        "            chunk = f.read()\n",
        "            if chunk:\n",
        "                print(chunk, end='', flush=True)\n",
        "                file_pos = f.tell()\n",
        "                if \"trycloudflare.com\" in chunk or \"your url is\" in chunk:\n",
        "                    for line in chunk.splitlines():\n",
        "                        if \"trycloudflare.com\" in line:\n",
        "                            url = line.split(\"http\")[1].strip()\n",
        "                            print(f\"\\n‚úÖ URL DE ACCESO: http{url}\")\n",
        "                            print(\"‚ö†Ô∏è La celda se cerrar√°, pero ComfyUI sigue funcionando en segundo plano.\")\n",
        "                            found = True\n",
        "                            break\n",
        "                        elif \"your url is\" in line:\n",
        "                            url = line.split(\"is:\")[1].strip()\n",
        "                            print(f\"\\n‚úÖ URL DE ACCESO: {url}\")\n",
        "                            print(\"‚ö†Ô∏è La celda se cerrar√°, pero ComfyUI sigue funcionando en segundo plano.\")\n",
        "                            found = True\n",
        "                            break\n",
        "    time.sleep(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "XLXxBZ_PVKPm"
      },
      "outputs": [],
      "source": [
        "#@title # üì¶ Instalar Modelos Personalizados Post-ejecuci√≥n\n",
        "#@markdown ---\n",
        "#@markdown ## Si necesitas instalar modelos despues de la ejecucion princpal lo puedes hacer desde aca:\n",
        "#@markdown Ingresa la lista de modelos en formato JSON en le recuadro de abajo, despues de colocar linea JSON ejecuta la celda.\n",
        "#@markdown > Puedes generar la cadena JSON con el formato correcto a trav√©s del siguiente proyecto de Google Apps Script:\n",
        "#@markdown > ### [üëâ Haz clic AQU√ç para abrir la herramienta](https://script.google.com/macros/s/AKfycbzmfmxfFRKvlnwlDuHNwOhWqEzYd90f1YKaQxSuFQ3o8HyQW_oHZZgPo9j4vUQlJ7ZI/exec)\n",
        "\n",
        "MODELOS_PERSONALIZADOS = \"[{\\\"id\\\":1,\\\"ruta\\\":\\\"vae/ae.safetensors\\\",\\\"link\\\":\\\"https://huggingface.co/Comfy-Org/Lumina_Image_2.0_Repackaged/resolve/main/split_files/vae/ae.safetensors\\\",\\\"workflow\\\":\\\"image_omnigen2_image_edit\\\"},{\\\"id\\\":2,\\\"ruta\\\":\\\"text_encoders/qwen_2.5_vl_fp16.safetensors\\\",\\\"link\\\":\\\"https://huggingface.co/Comfy-Org/Omnigen2_ComfyUI_repackaged/resolve/main/split_files/text_encoders/qwen_2.5_vl_fp16.safetensors\\\",\\\"workflow\\\":\\\"image_omnigen2_image_edit\\\"},{\\\"id\\\":3,\\\"ruta\\\":\\\"diffusion_models/omnigen2_fp16.safetensors\\\",\\\"link\\\":\\\"https://huggingface.co/Comfy-Org/Omnigen2_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/omnigen2_fp16.safetensors\\\",\\\"workflow\\\":\\\"image_omnigen2_image_edit\\\"}]\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown <font color='#888'>Nota: Aseg√∫rate de haber ejecutado primero la celda principal para definir la ruta de `WORKSPACE`.</font>\n",
        "\n",
        "import json\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "def descargar_modelos_custom():\n",
        "    if not MODELOS_PERSONALIZADOS or MODELOS_PERSONALIZADOS == \"[]\":\n",
        "        print(\"‚ÑπÔ∏è No se ingresaron modelos para descargar.\")\n",
        "        return\n",
        "\n",
        "    # Intentar cargar el token de Civitai si existe\n",
        "    try:\n",
        "        CIVITAI_API_TOKEN = userdata.get('CIVITAI_API_TOKEN')\n",
        "    except:\n",
        "        CIVITAI_API_TOKEN = \"\"\n",
        "        print(\"‚ö†Ô∏è Civitai Token no configurado en 'Secrets'.\")\n",
        "\n",
        "    def download_logic(url, filename=None):\n",
        "        auth_token = f\"?token={CIVITAI_API_TOKEN}\" if CIVITAI_API_TOKEN and \"civitai.com\" in url else \"\"\n",
        "        output_cmd = f\"-o {filename}\" if filename else \"\"\n",
        "\n",
        "        # Usamos aria2 para descargas r√°pidas y resilientes\n",
        "        !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{url}{auth_token}\" {output_cmd}\n",
        "\n",
        "    print(\"‚¨áÔ∏è Iniciando descarga de modelos personalizados...\")\n",
        "\n",
        "    try:\n",
        "        custom_models = json.loads(MODELOS_PERSONALIZADOS)\n",
        "        if isinstance(custom_models, list):\n",
        "            for model in custom_models:\n",
        "                path_str = model.get(\"ruta\", \"\").strip()\n",
        "                url = model.get(\"link\", \"\").strip()\n",
        "\n",
        "                if path_str and url:\n",
        "                    # Separar carpeta y nombre de archivo\n",
        "                    if \"/\" in path_str:\n",
        "                        folder_part, filename_part = path_str.rsplit(\"/\", 1)\n",
        "                    else:\n",
        "                        folder_part = \"checkpoints\" # Carpeta por defecto\n",
        "                        filename_part = path_str\n",
        "\n",
        "                    # Construir ruta completa\n",
        "                    target_path = os.path.join(WORKSPACE, \"models\", folder_part)\n",
        "                    os.makedirs(target_path, exist_ok=True)\n",
        "\n",
        "                    %cd {target_path}\n",
        "                    print(f\"üì• Descargando: {filename_part} en models/{folder_part}\")\n",
        "                    download_logic(url, filename_part)\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è Saltando entrada inv√°lida: {model}\")\n",
        "\n",
        "        print(\"\\n‚úÖ ¬°Proceso de descarga finalizado!\")\n",
        "        %cd {WORKSPACE}\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ùå Error: El contenido de MODELOS_PERSONALIZADOS no es un JSON v√°lido.\")\n",
        "    except NameError:\n",
        "        print(\"‚ùå Error: La variable 'WORKSPACE' no est√° definida. Ejecuta primero la celda de instalaci√≥n.\")\n",
        "\n",
        "descargar_modelos_custom()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1caIkpEgKAbM"
      },
      "outputs": [],
      "source": [
        "#@title üñ•Ô∏è Terminal Web Interactiva (ttyd)\n",
        "#@markdown Esta celda abrir√° una terminal en una pesta√±a nueva del navegador.\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "def setup_terminal():\n",
        "    print(\"üì¶ Instalando dependencias (ttyd y localtunnel)...\")\n",
        "    # Instalar ttyd\n",
        "    !apt-get update -qq && apt-get install -y -qq ttyd > /dev/null\n",
        "    # Instalar localtunnel si no est√°\n",
        "    if not os.path.exists(\"/usr/local/bin/lt\"):\n",
        "        !npm install -g localtunnel > /dev/null\n",
        "\n",
        "    # Puerto para la terminal\n",
        "    PORT = 7681\n",
        "\n",
        "    # 1. Obtener la IP p√∫blica (necesaria para la pantalla de seguridad de LocalTunnel)\n",
        "    try:\n",
        "        ip = urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip()\n",
        "    except:\n",
        "        ip = \"No se pudo obtener la IP\"\n",
        "\n",
        "    # 2. Iniciar ttyd en segundo plano\n",
        "    # Usamos 'bash' para que sea una terminal completa\n",
        "    subprocess.Popen([\"ttyd\", \"-p\", str(PORT), \"bash\"],\n",
        "                     stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "    print(\"‚è≥ Iniciando t√∫nel...\")\n",
        "    time.sleep(2)\n",
        "\n",
        "    # 3. Iniciar LocalTunnel\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"üîë PASSWORD/IP DE ACCESO: {ip}\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"üëâ Instrucciones:\")\n",
        "    print(\"1. Haz clic en el enlace 'url is:' que aparecer√° abajo.\")\n",
        "    print(f\"2. Pega la IP ({ip}) en la caja de texto que dice 'Endpoint IP'.\")\n",
        "    print(\"3. ¬°Listo! Ya tienes control total de la m√°quina.\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Ejecutar localtunnel\n",
        "    !lt --port {PORT}\n",
        "\n",
        "setup_terminal()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
